{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Testing From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "import time\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from gensim import corpora, utils\n",
    "from gensim.models.wrappers.dtmmodel import DtmModel\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def s_one_pre(topics):\n",
    "    \"\"\"\n",
    "    This function performs s_one_pre segmentation on a list of topics.\n",
    "    s_one_pre segmentation is defined as: s_one_pre = {(W', W*) | W' = {w_i};\n",
    "                                                                  W* = {w_j}; w_i, w_j belongs to W; i > j}\n",
    "    Example:\n",
    "        >>> topics = [np.array([1, 2, 3]), np.array([4, 5, 6])]\n",
    "        >>> s_one_pre(topics)\n",
    "        [[(2, 1), (3, 1), (3, 2)], [(5, 4), (6, 4), (6, 5)]]\n",
    "    Args:\n",
    "    ----\n",
    "    topics : list of topics obtained from an algorithm such as LDA. Is a list such as [array([ 9, 10, 11]), array([ 9, 10,  7]), ...]\n",
    "    Returns:\n",
    "    -------\n",
    "    s_one_pre : list of list of (W', W*) tuples for all unique topic ids\n",
    "    \"\"\"\n",
    "    s_one_pre = []\n",
    "\n",
    "    for top_words in topics:\n",
    "        s_one_pre_t = []\n",
    "        for w_prime_index, w_prime in enumerate(top_words[1:]):\n",
    "            for w_star in top_words[:w_prime_index + 1]:\n",
    "                s_one_pre_t.append((w_prime, w_star))\n",
    "        s_one_pre.append(s_one_pre_t)\n",
    "\n",
    "    return s_one_pre\n",
    "\n",
    "def s_one_one(topics):\n",
    "    \"\"\"\n",
    "    This function performs s_one_one segmentation on a list of topics.\n",
    "    s_one_one segmentation is defined as: s_one_one = {(W', W*) | W' = {w_i};\n",
    "                                                                  W* = {w_j}; w_i, w_j belongs to W; i != j}\n",
    "    Example:\n",
    "        >>> topics = [np.array([1, 2, 3]), np.array([4, 5, 6])]\n",
    "        >>> s_one_pre(topics)\n",
    "        [[(1, 2), (1, 3), (2, 1), (2, 3), (3, 1), (3, 2)], [(4, 5), (4, 6), (5, 4), (5, 6), (6, 4), (6, 5)]]\n",
    "    Args:\n",
    "    ----\n",
    "    topics : list of topics obtained from an algorithm such as LDA. Is a list such as [array([ 9, 10, 11]), array([ 9, 10,  7]), ...]\n",
    "    Returns:\n",
    "    -------\n",
    "    s_one_one : list of list of (W', W*) tuples for all unique topic ids\n",
    "    \"\"\"\n",
    "    s_one_one = []\n",
    "\n",
    "    for top_words in topics:\n",
    "        s_one_one_t = []\n",
    "        for w_prime_index, w_prime in enumerate(top_words):\n",
    "            for w_star_index, w_star in enumerate(top_words):\n",
    "                if w_prime_index == w_star_index:\n",
    "                    continue\n",
    "                else:\n",
    "                    s_one_one_t.append((w_prime, w_star))\n",
    "        s_one_one.append(s_one_one_t)\n",
    "\n",
    "    return s_one_one\n",
    "\n",
    "def s_one_set(topics):\n",
    "    \"\"\"\n",
    "    This function performs s_one_set segmentation on a list of topics.\n",
    "    s_one_set segmentation is defined as: s_one_set = {(W', W*) | W' = {w_i}; w_i belongs to W;\n",
    "                                                                  W* = W}\n",
    "    Example:\n",
    "        >>> topics = [np.array([9, 10, 7])\n",
    "        >>> s_one_set(topics)\n",
    "        [[(9, array([ 9, 10,  7])),\n",
    "          (10, array([ 9, 10,  7])),\n",
    "          (7, array([ 9, 10,  7]))]]\n",
    "    Args:\n",
    "    ----\n",
    "    topics : list of topics obtained from an algorithm such as LDA. Is a list such as [array([ 9, 10, 11]), array([ 9, 10,  7]), ...]\n",
    "    Returns:\n",
    "    -------\n",
    "    s_one_set : list of list of (W', W*) tuples for all unique topic ids.\n",
    "    \"\"\"\n",
    "    s_one_set = []\n",
    "\n",
    "    for top_words in topics:\n",
    "        s_one_set_t = []\n",
    "        for w_prime in top_words:\n",
    "            s_one_set_t.append((w_prime, top_words))\n",
    "        s_one_set.append(s_one_set_t)\n",
    "\n",
    "    return s_one_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPSILON = 1e-12 \n",
    "def log_ratio_measure(segmented_topics, per_topic_postings, num_docs, normalize=False):\n",
    "    \"\"\"\n",
    "    If normalize=False:\n",
    "        Popularly known as PMI.\n",
    "        This function calculates the log-ratio-measure which is used by\n",
    "        coherence measures such as c_v.\n",
    "        This is defined as: m_lr(S_i) = log[(P(W', W*) + e) / (P(W') * P(W*))]\n",
    "    If normalize=True:\n",
    "        This function calculates the normalized-log-ratio-measure, popularly knowns as\n",
    "        NPMI which is used by coherence measures such as c_v.\n",
    "        This is defined as: m_nlr(S_i) = m_lr(S_i) / -log[P(W', W*) + e]\n",
    "    Args:\n",
    "    ----\n",
    "    segmented topics : Output from the segmentation module of the segmented topics. Is a list of list of tuples.\n",
    "    per_topic_postings : Output from the probability_estimation module. Is a dictionary of the posting list of all topics\n",
    "    num_docs : Total number of documents in corpus. Used for calculating probability.\n",
    "    Returns:\n",
    "    -------\n",
    "    m_lr : List of log ratio measures on each set in segmented topics.\n",
    "    \"\"\"\n",
    "    m_lr = []\n",
    "    for s_i in segmented_topics:\n",
    "        for w_prime, w_star in s_i:\n",
    "            w_prime_docs = per_topic_postings[w_prime]\n",
    "            w_star_docs = per_topic_postings[w_star]\n",
    "            co_docs = w_prime_docs.intersection(w_star_docs)\n",
    "            if normalize:\n",
    "                # For normalized log ratio measure\n",
    "                numerator = log_ratio_measure([[(w_prime, w_star)]], per_topic_postings, num_docs)[0]\n",
    "                co_doc_prob = len(co_docs) / float(num_docs)\n",
    "                m_lr_i = numerator / (-np.log(co_doc_prob + EPSILON))\n",
    "            else:\n",
    "                # For log ratio measure without normalization\n",
    "                numerator = (len(co_docs) / float(num_docs)) + EPSILON\n",
    "                denominator = (len(w_prime_docs) / float(num_docs)) * (len(w_star_docs) / float(num_docs))\n",
    "                m_lr_i = np.log(numerator / denominator)\n",
    "            m_lr.append(m_lr_i)\n",
    "\n",
    "    return m_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from gensim import interfaces\n",
    "from gensim.topic_coherence import (probability_estimation,\n",
    "                                    direct_confirmation_measure, indirect_confirmation_measure,\n",
    "                                    aggregation)\n",
    "from gensim.matutils import argsort\n",
    "from gensim.utils import is_corpus, FakeDict\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.wrappers import LdaVowpalWabbit, LdaMallet\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "boolean_document_based = ['u_mass']\n",
    "sliding_window_based = ['c_v', 'c_uci', 'c_npmi']\n",
    "make_pipeline = namedtuple('Coherence_Measure', 'seg, prob, conf, aggr')\n",
    "\n",
    "coherence_dict = {\n",
    "    'u_mass': make_pipeline(s_one_pre,\n",
    "                            probability_estimation.p_boolean_document,\n",
    "                            direct_confirmation_measure.log_conditional_probability,\n",
    "                            aggregation.arithmetic_mean),\n",
    "    'c_v': make_pipeline(s_one_set,\n",
    "                         probability_estimation.p_boolean_sliding_window,\n",
    "                         indirect_confirmation_measure.cosine_similarity,\n",
    "                         aggregation.arithmetic_mean),\n",
    "    'c_uci': make_pipeline(s_one_one,\n",
    "                           probability_estimation.p_boolean_sliding_window,log_ratio_measure,\n",
    "                           aggregation.arithmetic_mean),\n",
    "    'c_npmi': make_pipeline(s_one_one,\n",
    "                            probability_estimation.p_boolean_sliding_window,log_ratio_measure,\n",
    "                            aggregation.arithmetic_mean),\n",
    "}\n",
    "\n",
    "sliding_windows_dict = {\n",
    "    'c_v': 110,\n",
    "    'c_uci': 10,\n",
    "    'c_npmi': 10\n",
    "}\n",
    "\n",
    "class CoherenceModel(interfaces.TransformationABC):\n",
    "    \"\"\"\n",
    "    Objects of this class allow for building and maintaining a model for topic\n",
    "    coherence.\n",
    "    The main methods are:\n",
    "    1. constructor, which initializes the four stage pipeline by accepting a coherence measure,\n",
    "    2. the ``get_coherence()`` method, which returns the topic coherence.\n",
    "    One way of using this feature is through providing a trained topic model. A dictionary has to be explicitly\n",
    "    provided if the model does not contain a dictionary already.\n",
    "    >>> cm = CoherenceModel(model=tm, corpus=corpus, coherence='u_mass')  # tm is the trained topic model\n",
    "    >>> cm.get_coherence()\n",
    "    Another way of using this feature is through providing tokenized topics such as:\n",
    "    >>> topics = [['human', 'computer', 'system', 'interface'],\n",
    "                  ['graph', 'minors', 'trees', 'eps']]\n",
    "    >>> cm = CoherenceModel(topics=topics, corpus=corpus, dictionary=dictionary, coherence='u_mass') # note that a dictionary has to be provided.\n",
    "    >>> cm.get_coherence()\n",
    "    Model persistency is achieved via its load/save methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, model=None, topics=None, texts=None, corpus=None, dictionary=None, window_size=None, coherence='c_v', topn=10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        ----\n",
    "        model : Pre-trained topic model. Should be provided if topics is not provided.\n",
    "        topics : List of tokenized topics. If this is preferred over model, dictionary should be provided.\n",
    "                 eg. topics = [['human', 'machine', 'computer', 'interface'],\n",
    "                               ['graph', 'trees', 'binary', 'widths']]\n",
    "        texts : Tokenized texts. Needed for coherence models that use sliding window based probability estimator.\n",
    "        corpus : Gensim document corpus.\n",
    "        dictionary : Gensim dictionary mapping of id word to create corpus. If model.id2word is present, this is not needed.\n",
    "                     If both are provided, dictionary will be used.\n",
    "        window_size : Is the size of the window to be used for coherence measures using boolean sliding window as their\n",
    "                      probability estimator. For 'u_mass' this doesn't matter.\n",
    "                      If left 'None' the default window sizes are used which are:\n",
    "                      'c_v' : 110\n",
    "                      'c_uci' : 10\n",
    "                      'c_npmi' : 10\n",
    "        coherence : Coherence measure to be used. Supported values are:\n",
    "                    'u_mass'\n",
    "                    'c_v'\n",
    "                    'c_uci' also popularly known as c_pmi\n",
    "                    'c_npmi'\n",
    "                    For 'u_mass' corpus should be provided. If texts is provided, it will be converted to corpus using the dictionary.\n",
    "                    For 'c_v', 'c_uci' and 'c_npmi' texts should be provided. Corpus is not needed.\n",
    "        topn : Integer corresponding to the number of top words to be extracted from each topic.\n",
    "        \"\"\"\n",
    "        if model is None and topics is None:\n",
    "            raise ValueError(\"One of model or topics has to be provided.\")\n",
    "        elif topics is not None and dictionary is None:\n",
    "            raise ValueError(\"dictionary has to be provided if topics are to be used.\")\n",
    "        if texts is None and corpus is None:\n",
    "            raise ValueError(\"One of texts or corpus has to be provided.\")\n",
    "        # Check if associated dictionary is provided.\n",
    "        if dictionary is None:\n",
    "            if isinstance(model.id2word, FakeDict):\n",
    "                raise ValueError(\"The associated dictionary should be provided with the corpus or 'id2word' for topic model\"\n",
    "                                 \" should be set as the associated dictionary.\")\n",
    "            else:\n",
    "                self.dictionary = model.id2word\n",
    "        else:\n",
    "            self.dictionary = dictionary\n",
    "        # Check for correct inputs for u_mass coherence measure.\n",
    "        if coherence in boolean_document_based:\n",
    "            if is_corpus(corpus)[0]:\n",
    "                self.corpus = corpus\n",
    "            elif texts is not None:\n",
    "                self.texts = texts\n",
    "                self.corpus = [self.dictionary.doc2bow(text) for text in self.texts]\n",
    "            else:\n",
    "                raise ValueError(\"Either 'corpus' with 'dictionary' or 'texts' should be provided for %s coherence.\" % coherence)\n",
    "        # Check for correct inputs for c_v coherence measure.\n",
    "        elif coherence in sliding_window_based:\n",
    "            self.window_size = window_size\n",
    "            if texts is None:\n",
    "                raise ValueError(\"'texts' should be provided for %s coherence.\" % coherence)\n",
    "            else:\n",
    "                self.texts = texts\n",
    "        else:\n",
    "            raise ValueError(\"%s coherence is not currently supported.\" % coherence)\n",
    "        self.topn = topn\n",
    "        self.model = model\n",
    "        if model is not None:\n",
    "            self.topics = self._get_topics()\n",
    "        elif topics is not None:\n",
    "            self.topics = []\n",
    "            for topic in topics:\n",
    "                t_i = []\n",
    "                for n, _ in enumerate(topic):\n",
    "                    t_i.append(dictionary.token2id[topic[n]])\n",
    "                self.topics.append(np.array(t_i))\n",
    "        self.coherence = coherence\n",
    "\n",
    "    def __str__(self):\n",
    "        return coherence_dict[self.coherence].__str__()\n",
    "\n",
    "    def _get_topics(self):\n",
    "        \"\"\"Internal helper function to return topics from a trained topic model.\"\"\"\n",
    "        topics = []\n",
    "        if isinstance(self.model, LdaModel):\n",
    "            for topic in self.model.state.get_lambda():\n",
    "                bestn = argsort(topic, topn=self.topn, reverse=True)\n",
    "                topics.append(bestn)\n",
    "        elif isinstance(self.model, LdaVowpalWabbit):\n",
    "            for topic in self.model._get_topics():\n",
    "                bestn = argsort(topic, topn=self.topn, reverse=True)\n",
    "                topics.append(bestn)\n",
    "        elif isinstance(self.model, LdaMallet):\n",
    "            for topic in self.model.word_topics:\n",
    "                bestn = argsort(topic, topn=self.topn, reverse=True)\n",
    "                topics.append(bestn)\n",
    "        else:\n",
    "            raise ValueError(\"This topic model is not currently supported. Supported topic models are\"\n",
    "                             \"LdaModel, LdaVowpalWabbit and LdaMallet.\")\n",
    "        return topics\n",
    "\n",
    "    def get_coherence(self):\n",
    "        \"\"\"\n",
    "        Return coherence value based on pipeline parameters.\n",
    "        \"\"\"\n",
    "        measure = coherence_dict[self.coherence]\n",
    "        segmented_topics = measure.seg(self.topics)\n",
    "        if self.coherence in boolean_document_based:\n",
    "            per_topic_postings, num_docs = measure.prob(self.corpus, segmented_topics)\n",
    "            confirmed_measures = measure.conf(segmented_topics, per_topic_postings, num_docs)\n",
    "        elif self.coherence in sliding_window_based:\n",
    "            if self.window_size is not None:\n",
    "                self.window_size = sliding_windows_dict[self.coherence]\n",
    "            per_topic_postings, num_windows = measure.prob(texts=self.texts, segmented_topics=segmented_topics,\n",
    "                                                           dictionary=self.dictionary, window_size=self.window_size)\n",
    "            if self.coherence == 'c_v':\n",
    "                confirmed_measures = measure.conf(self.topics, segmented_topics, per_topic_postings, 'nlr', 1, num_windows)\n",
    "            else:\n",
    "                if self.coherence == 'c_npmi':\n",
    "                    normalize = True\n",
    "                else:\n",
    "                    # For c_uci\n",
    "                    normalize = False\n",
    "                confirmed_measures = measure.conf(segmented_topics, per_topic_postings, num_windows, normalize=normalize)\n",
    "        return measure.aggr(confirmed_measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [['human', 'interface', 'computer'],\n",
    "         ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "         ['eps', 'user', 'interface', 'system'],\n",
    "         ['system', 'human', 'system', 'eps'],\n",
    "         ['user', 'response', 'time'],\n",
    "         ['trees'],\n",
    "         ['graph', 'trees'],\n",
    "         ['graph', 'minors', 'trees'],\n",
    "         ['graph', 'minors', 'survey']]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    }
   ],
   "source": [
    "# make LDa model\n",
    "goodLdaModel = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, iterations=50, num_topics=2)\n",
    "badLdaModel = gensim.models.LdaModel(corpus=corpus, id2word=dictionary, iterations=1, num_topics=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# U_mass Coherence\n",
    "good_um = CoherenceModel(model=goodLdaModel, corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "bad_um = CoherenceModel(model=badLdaModel, corpus=corpus, dictionary=dictionary, coherence='u_mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C_V Coherence\n",
    "good_cv = CoherenceModel(model=goodLdaModel, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "bad_cv = CoherenceModel(model=badLdaModel, texts=texts, dictionary=dictionary, coherence='c_v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# C_UCI Coherence\n",
    "good_uci = CoherenceModel(model=goodLdaModel, texts=texts, dictionary=dictionary, coherence='c_uci')\n",
    "bad_uci = CoherenceModel(model=badLdaModel, texts=texts, dictionary=dictionary, coherence='c_uci')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# C_NPMI Coherence\n",
    "good_npmi = CoherenceModel(model=goodLdaModel, texts=texts, dictionary=dictionary, coherence='c_npmi')\n",
    "bad_npmi = CoherenceModel(model=badLdaModel, texts=texts, dictionary=dictionary, coherence='c_npmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "higher score is better\n",
      "good model umass:  -14.0810486906\n",
      "bad model umass:  -14.7170181547\n",
      "good model cv:  0.385963126348\n",
      "bad model cv:  0.341017550076\n",
      "good model cuci:  -12.7031159941\n",
      "bad model cuci:  -13.2760131081\n",
      "good model cnpmi:  -0.288386836533\n",
      "bad model cnpmi:  -0.327176330368\n"
     ]
    }
   ],
   "source": [
    "print(\"higher score is better\")\n",
    "print(\"good model umass: \", good_um.get_coherence())\n",
    "print(\"bad model umass: \", bad_um.get_coherence())\n",
    "print(\"good model cv: \", good_cv.get_coherence())\n",
    "print(\"bad model cv: \", bad_cv.get_coherence())\n",
    "print(\"good model cuci: \", good_uci.get_coherence())\n",
    "print(\"bad model cuci: \", bad_uci.get_coherence())\n",
    "print(\"good model cnpmi: \", good_npmi.get_coherence())\n",
    "print(\"bad model cnpmi: \", bad_npmi.get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Testing From Saved Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ":0: FutureWarning: IPython widgets are experimental and may change in the future.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "import os\n",
    "import pylab as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_MODELS_DIR = \"saved_models/\"\n",
    "key=\"Y02E_10_20\"\n",
    "m1 =\"DTM_model\"\n",
    "m2 =\"DIM_model\"\n",
    "dict_file = \"{}.dict\".format(key)\n",
    "corpus_file = \"{}.mm\".format(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArgSpec(args=['self', 'model', 'texts', 'corpus', 'dictionary', 'coherence'], varargs=None, keywords=None, defaults=(None, None, None, 'c_v'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspect\n",
    "inspect.getargspec(CoherenceModel.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get corpus\n",
    "corpus = gensim.corpora.MmCorpus(os.path.join(_MODELS_DIR, corpus_file))\n",
    "\n",
    "# get dictionary\n",
    "dictionary = gensim.corpora.Dictionary.load(os.path.join(_MODELS_DIR, dict_file))\n",
    "\n",
    "# get DTM model\n",
    "filehandler = open(_MODELS_DIR + m1 + \".obj\",'r')\n",
    "DTM = pickle.load(filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NEED TO GET TEXTS FROM SAVED CORPUS! \n",
    "# but it's serialized so you'll have to undo the serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "c_v = []\n",
    "for n, topic in enumerate(topics):\n",
    "    print n  # for personal monitoring purposes. sorry for this\n",
    "    try:\n",
    "        cm = CoherenceModel(topics=topic, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        c_v.append(cm.get_coherence())\n",
    "    except KeyError:\n",
    "        pass\n",
    "end = datetime.now()\n",
    "print \"Time taken: %s\" % (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coherence Testing From Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making dictionary\n",
      "318.760668039\n",
      "saving dictionary\n",
      "0.0668199062347\n",
      "Saving corpus\n",
      "156.107006073\n"
     ]
    }
   ],
   "source": [
    "from DTM_Pipeline import Pipeline\n",
    "\n",
    "# read in data\n",
    "# make corpus, texts etc.\n",
    "pl = Pipeline(key=\"Y02E_10_20\",m_type=\"DIM\",num_topics=10)\n",
    "pl.make_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stream_corp(path,approved_ids=None):\n",
    "    '''generator for iterating through lines of the data\n",
    "    '''\n",
    "    # regex for sanitizing the abstracts\n",
    "    html = re.compile(r'\\<[^\\>]*\\>')\n",
    "    nonan = re.compile(r'[^a-zA-Z ]')\n",
    "\n",
    "    # read file one line at a time and sanitize\n",
    "    for line in pd.read_csv(path,sep=',', chunksize=1):\n",
    "        if approved_ids == None:\n",
    "            line = line[\"appln_abstract\"].values[0]\n",
    "            line = nonan.sub(' ',html.sub('',str(line))).lower().split()\n",
    "            line = stem_doc(line) #apply lemmatization/stemming\n",
    "            yield line\n",
    "        else: # if there are approved_ids\n",
    "            if line[\"appln_id\"].values[0] in approved_ids:\n",
    "                line = line[\"appln_abstract\"].values[0]\n",
    "                line = nonan.sub(' ',html.sub('',str(line))).lower().split()\n",
    "                line = stem_doc(line) #apply lemmatization/stemming\n",
    "                yield line\n",
    "\n",
    "def get_time_seq(data_file, min_slice_size=None):\n",
    "    df =  pd.read_csv(data_file)\n",
    "    # Create dummy column\n",
    "    df[\"Y\"] = pd.DatetimeIndex(df[\"appln_filing_date\"]).to_period(\"A\")\n",
    "    # group by dummy column\n",
    "    groups = df.groupby(\"Y\")\n",
    "    # return sorted df and counts dict\n",
    "    #df = df.sort_values(\"appln_filing_date\")\n",
    "    approved_ids = None\n",
    "    if min_slice_size == None:\n",
    "        # count members of each group\n",
    "        counts = np.sort([[key,len(groups.groups[key])] for key in groups.groups.keys()], axis=0)\n",
    "        time_seq = list(counts[:,1])\n",
    "    else:\n",
    "        approved_ids = []\n",
    "        for group in groups.groups.iteritems():\n",
    "            if len(group[1]) >= min_slice_size:\n",
    "                approved_ids.append(df.loc[group[1]][\"appln_id\"].values[:min_slice_size])\n",
    "        time_seq = [min_slice_size]*len(approved_ids)\n",
    "    return time_seq, approved_ids  \n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    tag_to_type = {'J': wordnet.ADJ, 'V': wordnet.VERB, 'R': wordnet.ADV}\n",
    "    return tag_to_type.get(treebank_tag[:1], wordnet.NOUN)\n",
    "\n",
    "def stem_doc(doc):\n",
    "    lmtzr = WordNetLemmatizer()\n",
    "    tags = nltk.pos_tag(doc)\n",
    "    return [lmtzr.lemmatize(word, get_wordnet_pos(tag[1])) for word, tag in zip(doc, tags)]\n",
    "\n",
    "def progress(percent):\n",
    "    '''Just a progress bar to be printed to the screen\n",
    "    '''\n",
    "    width = 10\n",
    "    perc = np.floor(percent*width)\n",
    "    prog = \"=\"*perc + \">\"\n",
    "    if percent != 1:\n",
    "        print('\\r' + 'Progress: [{}] {}%'.format(prog.ljust(width+1), str(percent*100)),end=\"\")\n",
    "    else:\n",
    "        print('\\r' + 'Progress: [{}] {}%'.format(prog.ljust(width+1), str(percent*100)),end=\"\")\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "key=\"Y02E_10_20\"\n",
    "data_file = '../Data/{}.csv'.format(key)\n",
    "min_slice_size=200\n",
    "time_seq, approved_ids = get_time_seq(data_file, min_slice_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160.36295414\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "pl.corpus.get_texts()\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ivd = {v: k for k, v in pl.corpus.dictionary.token2id.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'force',\n",
       " u'force',\n",
       " u'type',\n",
       " u'type',\n",
       " u'provide',\n",
       " u'sliding',\n",
       " u'sliding',\n",
       " u'runner',\n",
       " u'runner',\n",
       " u'runner',\n",
       " u'plate',\n",
       " u'plate',\n",
       " u'plate',\n",
       " u'improve',\n",
       " u'improve',\n",
       " u'wheel',\n",
       " u'wheel',\n",
       " u'whereby',\n",
       " u'crown',\n",
       " u'support',\n",
       " u'form',\n",
       " u'centrifugal',\n",
       " u'centrifugal',\n",
       " u'water',\n",
       " u'water',\n",
       " u'water',\n",
       " u'water',\n",
       " u'water',\n",
       " u'water',\n",
       " u'water',\n",
       " u'produce',\n",
       " u'shield',\n",
       " u'shield',\n",
       " u'shield',\n",
       " u'shield',\n",
       " u'equip',\n",
       " u'pumping',\n",
       " u'pumping',\n",
       " u'chamber',\n",
       " u'chamber',\n",
       " u'francis',\n",
       " u'francis',\n",
       " u'act',\n",
       " u'project',\n",
       " u'project',\n",
       " u'reduce',\n",
       " u'shroud',\n",
       " u'revolve',\n",
       " u'revolve',\n",
       " u'method',\n",
       " u'since',\n",
       " u'plat',\n",
       " u'constitution',\n",
       " u'cause',\n",
       " u'part',\n",
       " u'wherein',\n",
       " u'spring',\n",
       " u'structure',\n",
       " u'leakage',\n",
       " u'purpose',\n",
       " u'sealed',\n",
       " u'efficiency',\n",
       " u'efficiency',\n",
       " u'leak',\n",
       " u'leak',\n",
       " u'leak',\n",
       " u'leak',\n",
       " u'outward',\n",
       " u'outward',\n",
       " u'radially']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = pl.corpus.docs[102]\n",
    "text = list(itertools.chain(*[[ivd[word[0]]]*word[1] for word in doc]))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = []\n",
    "for doc in pl.corpus.docs[:400]: \n",
    "    text = list(itertools.chain(*[[ivd[word[0]]]*word[1] for word in doc]))\n",
    "    texts.append(text)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_dictionary = Dictionary(texts)\n",
    "c_corpus = [c_dictionary.doc2bow(text) for text in texts]\n",
    "c_ts = [200,200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create training/validation/testing splits.\n",
    "#train = pl.corpus.docs[:400]  # [:4800]\n",
    "#val = pl.corpus.docs[400:600] # [4800:6000]\n",
    "#test = pl.corpus.docs[600:800] # [6000:]\n",
    "\n",
    "#train_ts = [200,200]\n",
    "#val_ts = [200]\n",
    "#test_ts = [200]\n",
    "\n",
    "#train_texts = texts[:400]\n",
    "#val_texts = texts[400:600]\n",
    "#test_texts = texts[600:800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stoplist = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "#train_dict = make_dict(train, stoplist=stoplist)\n",
    "#val_dict = make_dict(val, stoplist=stoplist)\n",
    "#test_dict = make_dict(test, stoplist=stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.ldamodel:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
     ]
    }
   ],
   "source": [
    "nt = 2\n",
    "# Static LDA\n",
    "#lda = gensim.models.LdaModel(train, id2word=pl.corpus.dictionary, num_topics=nt)\n",
    "lda = gensim.models.LdaModel(corpus=c_corpus, id2word=c_dictionary, iterations=10, num_topics=nt)\n",
    "\n",
    "# get and strip down the topics\n",
    "lda_tops = lda.print_topics(num_topics=nt, num_words=10)\n",
    "lda_tops = [\" \".join(re.findall(\"[a-zA-Z]+\", lda_tops[i][1])).split() for i in range(len(lda_tops))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "dtm_home = os.environ.get('DTM_HOME', \"dtm-master\")\n",
    "dtm_path = os.path.join(dtm_home, 'bin', 'dtm-darwin64') if dtm_home else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DTM\n",
    "#dtm = DtmModel(dtm_path,train,train_ts,num_topics=nt,id2word=pl.corpus.dictionary,initialize_lda=True)\n",
    "dtm = DtmModel(dtm_path,c_corpus,c_ts,num_topics=nt,id2word=c_dictionary,initialize_lda=True)\n",
    "\n",
    "# get dtm topics\n",
    "dtm_tops = dtm.show_topics(topics=nt,times=1, topn=10)\n",
    "dtm_tops = [\" \".join(re.findall(\"[a-zA-Z]+\", dtm_tops[i])).split() for i in range(len(dtm_tops))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DIM\n",
    "dim = DtmModel(dtm_path,c_corpus,c_ts,num_topics=nt,model=\"fixed\",id2word=c_dictionary,initialize_lda=True)\n",
    "\n",
    "# get dim topics\n",
    "dim_tops = dim.show_topics(topics=nt, times=1, topn=10)\n",
    "dim_tops = [\" \".join(re.findall(\"[a-zA-Z]+\", dim_tops[i])).split() for i in range(len(dim_tops))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test coherence\n",
    "def test_coherence(topics,corpus,texts,dictionary):\n",
    "    coherences = [\"u_mass\",\"c_v\",\"c_uci\",\"c_npmi\"]\n",
    "    coh = []\n",
    "    for c in coherences:\n",
    "        if c == \"u_mass\":\n",
    "            temp = CoherenceModel(topics=topics,corpus=corpus, dictionary=dictionary, coherence=c)\n",
    "        else:\n",
    "            temp = CoherenceModel(topics=topics,texts=texts, dictionary=dictionary, coherence=c)\n",
    "        coh.append(temp.get_coherence())\n",
    "    return coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#lda coherence\n",
    "lda_coherences = test_coherence(lda_tops,c_corpus,texts,c_dictionary)\n",
    "dtm_coherences = test_coherence(dtm_tops,c_corpus,texts,c_dictionary)\n",
    "dim_coherences = test_coherence(dim_tops,c_corpus,texts,c_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>u_mass</th>\n",
       "      <th>c_v</th>\n",
       "      <th>c_uci</th>\n",
       "      <th>c_npmi</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.973999</td>\n",
       "      <td>0.418465</td>\n",
       "      <td>0.057484</td>\n",
       "      <td>0.044925</td>\n",
       "      <td>LDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.972552</td>\n",
       "      <td>0.469893</td>\n",
       "      <td>0.102802</td>\n",
       "      <td>0.065428</td>\n",
       "      <td>DTM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.973156</td>\n",
       "      <td>0.478858</td>\n",
       "      <td>0.141175</td>\n",
       "      <td>0.069008</td>\n",
       "      <td>DIM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     u_mass       c_v     c_uci    c_npmi model\n",
       "0 -0.973999  0.418465  0.057484  0.044925   LDA\n",
       "1 -0.972552  0.469893  0.102802  0.065428   DTM\n",
       "2 -0.973156  0.478858  0.141175  0.069008   DIM"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherences = pd.DataFrame([lda_coherences,dtm_coherences,dim_coherences])\n",
    "coherences.columns = [\"u_mass\",\"c_v\",\"c_uci\",\"c_npmi\"]\n",
    "coherences[\"model\"] = [\"LDA\",\"DTM\",\"DIM\"]\n",
    "coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get corpus\n",
    "#corpus = gensim.corpora.MmCorpus(os.path.join(_MODELS_DIR, corpus_file))\n",
    "#list(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#texts = []\n",
    "#for doc in corpus: \n",
    "#    text = list(itertools.chain(*[[ivd[word[0]]]*int(word[1]) for word in doc]))\n",
    "#    texts.append(text)\n",
    "#len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Parameter tuning LDA\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
